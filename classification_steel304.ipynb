{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TLT Image Classification -- Steel 304 dataset\n",
    "\n",
    "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n",
    "\n",
    "Transfer Learning Toolkit (TLT) is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
    "This notebooks uses TLT to train a Neural Network on iamges of various defects in steel welding. \n",
    "\n",
    "<img align=\"center\" src=\"https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png\" width=\"1080\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Set up env variables and map drives <a class=\"anchor\" id=\"head-0\"></a>\n",
    "When using the purpose-built pretrained models from NGC, please make sure to set the `$KEY` environment variable to the key as mentioned in the model overview. Failing to do so, can lead to errors when trying to load them as pretrained models.\n",
    "\n",
    "The following notebook requires the user to set an env variable called the `$LOCAL_PROJECT_DIR` as the path to the users workspace. Please note that the dataset to run this notebook is expected to reside in the `$LOCAL_PROJECT_DIR/data`, while the TLT experiment generated collaterals will be output to `$LOCAL_PROJECT_DIR/classification`. More information on how to set up the dataset and the supported steps in the TLT workflow are provided in the subsequent cells.\n",
    "\n",
    "*Note: Please make sure to remove any stray artifacts/files from the `$USER_EXPERIMENT_DIR` or `$DATA_DOWNLOAD_DIR` paths as mentioned below, that may have been generated from previous experiments. Having checkpoint files etc may interfere with creating a training graph for a new experiment.*\n",
    "\n",
    "*Note: This notebook currently is by default set up to run training using 1 GPU. To use more GPU's please update the env variable `$NUM_GPUS` accordingly*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up env variables for cleaner command line commands.\n",
    "import os\n",
    "\n",
    "# insert your personal key to NGC, it can be obtained for free\n",
    "#%env KEY=\n",
    "%env NUM_GPUS=1\n",
    "%env USER_EXPERIMENT_DIR=/workspace/tlt-experiments/classification\n",
    "%env DATA_DOWNLOAD_DIR=/workspace/tlt-experiments/data\n",
    "\n",
    "# Set this path if you don't run the notebook from the samples directory.\n",
    "%env NOTEBOOK_ROOT=/mnt/sdb/AI/TLT/tlt_cv_samples_v1.0.2/classification_steel304\n",
    "\n",
    "# Please define this local project directory that needs to be mapped to the TLT docker session.\n",
    "# The dataset expected to be present in $LOCAL_PROJECT_DIR/data, while the results for the steps\n",
    "# in this notebook will be stored at $LOCAL_PROJECT_DIR/classification\n",
    "# !PLEASE MAKE SURE TO UPDATE THIS PATH!.\n",
    "\n",
    "os.environ[\"LOCAL_PROJECT_DIR\"] = \"/mnt/sdb/AI/TLT/tlt_cv_samples_v1.0.2/classification_steel304\"\n",
    "\n",
    "os.environ[\"LOCAL_DATA_DIR\"] = os.path.join(\n",
    "    os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()),\n",
    "    \"data\"\n",
    ")\n",
    "os.environ[\"LOCAL_EXPERIMENT_DIR\"] = os.path.join(\n",
    "    os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()),\n",
    "    \"classification\"\n",
    ")\n",
    "\n",
    "# The sample spec files are present in the same path as the downloaded samples.\n",
    "os.environ[\"LOCAL_SPECS_DIR\"] = os.path.join(\n",
    "    os.getenv(\"NOTEBOOK_ROOT\", os.getcwd()),\n",
    "    \"specs\"\n",
    ")\n",
    "%env SPECS_DIR=/workspace/tlt-experiments/classification/specs\n",
    "\n",
    "# Showing list of specification files.\n",
    "!ls -rlt $LOCAL_SPECS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below maps the project directory on your local host to a workspace directory in the TLT docker instance, so that the data and the results are mapped from outside to inside of the docker instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping up the local directories to the TLT docker.\n",
    "import json\n",
    "import os\n",
    "mounts_file = os.path.expanduser(\"~/.tlt_mounts.json\")\n",
    "\n",
    "# Define the dictionary with the mapped drives\n",
    "drive_map = {\n",
    "    \"Mounts\": [\n",
    "        # Mapping the data directory\n",
    "        {\n",
    "            \"source\": os.environ[\"LOCAL_PROJECT_DIR\"],\n",
    "            \"destination\": \"/workspace/tlt-experiments\"\n",
    "        },\n",
    "        # Mapping the specs directory.\n",
    "        {\n",
    "            \"source\": os.environ[\"LOCAL_SPECS_DIR\"],\n",
    "            \"destination\": os.environ[\"SPECS_DIR\"]\n",
    "        },\n",
    "    ],\n",
    "    \"DockerOptions\":{\n",
    "        \"user\": \"{}:{}\".format(os.getuid(), os.getgid())\n",
    "    }\n",
    "}\n",
    "\n",
    "# Writing the mounts file.\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(drive_map, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ~/.tlt_mounts.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP this cell IF you have already installed the TLT launcher.\n",
    "!pip3 install nvidia-pyindex\n",
    "!pip3 install nvidia-tlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the versions of the TLT launcher\n",
    "!tlt info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare datasets and pre-trained model <a class=\"anchor\" id=\"head-2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following dataset: https://www.kaggle.com/danielbacioiu/tig-stainless-steel-304\n",
    "The Dataset stems from Researchers at the University of Birmingham and describes various defects that occur in steel welding. \n",
    "The dataset comes already split into a train, validation and test data set. It is important that this split remains intact as the data stems from continous camera runs. Interleving these camera runs would yield good model performance but bad performance in real world applications. \n",
    "For further infromation on the dataset take a look at their publication: http://www.sciencedirect.com/science/article/pii/S0963869518305942"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data from kaggle into the LOCAL_DATA_DIR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "DATA_DIR = os.environ.get('LOCAL_DATA_DIR')\n",
    "print(DATA_DIR)\n",
    "\n",
    "!ls $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unpack the data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the unpacking \n",
    "!ls $LOCAL_DATA_DIR/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Split the dataset into train/val/test <a class=\"anchor\" id=\"head-1-1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TLT accepts the data in the following format: \n",
    "```\n",
    "|--dataset_root:\n",
    "    |--train\n",
    "        |--audi:\n",
    "            |--1.jpg\n",
    "            |--2.jpg\n",
    "        |--bmw:\n",
    "            |--01.jpg\n",
    "            |--02.jpg\n",
    "    |--val\n",
    "        |--audi:\n",
    "            |--3.jpg\n",
    "            |--4.jpg\n",
    "        |--bmw:\n",
    "            |--03.jpg\n",
    "            |--04.jpg\n",
    "    |--test\n",
    "        |--audi:\n",
    "            |--5.jpg\n",
    "            |--6.jpg\n",
    "        |--bmw:\n",
    "            |--05.jpg\n",
    "            |--06.jpg\n",
    "```            \n",
    "So we have to bring our data into this format. The dataset we want use is split into train/validation/test. We have to use the json file with the labels to build this new directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install pip requirements\n",
    "!pip3 install tqdm\n",
    "!pip3 install matplotlib==3.3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change source dir to the source of the data and make sure you change the *.suffix in img_list to the correct suffix of the image type you are using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os \n",
    "from fnmatch import fnmatch\n",
    "import shutil\n",
    "import glob \n",
    "from collections import Counter\n",
    "\n",
    "DATA_DIR=os.environ.get('LOCAL_DATA_DIR')\n",
    "TARGET_DIR=os.path.join(DATA_DIR,'split')\n",
    "\n",
    "PATH_SOURCE = os.path.join(DATA_DIR,'ss304')\n",
    "\n",
    "label_list=['good_weld','burn_through','contamination','lack_of_fusion','lack_of_shielding_gas','high_travel_speed']    \n",
    "dataset_list=['valid','train','test']\n",
    "\n",
    "#make the split directory \n",
    "if not os.path.exists(TARGET_DIR):\n",
    "        os.mkdir(TARGET_DIR)\n",
    "\n",
    "for data_set in dataset_list:\n",
    "    counter1 = 0\n",
    "    with open(PATH_SOURCE + '/' + data_set + '/' + data_set + '.json') as file:\n",
    "        f_json = json.load(file)\n",
    "    \n",
    "    distribution_classes = Counter(f_json.values())\n",
    "    nClass = len(f_json)\n",
    "    \n",
    "    print(distribution_classes)\n",
    "    print(distribution_classes[0])\n",
    "    \n",
    "    #make the target directories for classes    \n",
    "    PATH_TARGET_DATASET = os.path.join(TARGET_DIR,data_set)    \n",
    "    if not os.path.exists(PATH_TARGET_DATASET):\n",
    "        os.mkdir(PATH_TARGET_DATASET)\n",
    "\n",
    "    for label in label_list:\n",
    "        label_path = os.path.join(TARGET_DIR,data_set,label)\n",
    "        if not os.path.exists(label_path):\n",
    "            os.mkdir(label_path)\n",
    "\n",
    "    suffix = '.png'\n",
    "    pattern_test = os.path.join(DATA_DIR,'ss304',data_set,'*/*')\n",
    "    print(pattern_test+suffix)\n",
    "\n",
    "    #get a image in the directory train,test,valid and look it up in the dictionary \n",
    "    #then put it in the correct new directory based on its label\n",
    "    for img in glob.glob(pattern_test+suffix):\n",
    "        image = img.replace(PATH_SOURCE+'/'+data_set+'/','')\n",
    "        \n",
    " \n",
    "        #can be used to oversample the dataset, did not improve the performance by a lot/much longer training time\n",
    "        for j in range(1):#int((nClass / distribution_classes[f_json[image]])) % distribution_classes[0] + 1):\n",
    "            copy_path=os.path.join(TARGET_DIR,data_set,label_list[f_json[image]])\n",
    "            image_mod = image.replace('/','')\n",
    "            shutil.copy(img,copy_path+'/'+ str(j) +image_mod)   \n",
    "            counter1 +=1\n",
    "    \n",
    "    print(\"Number of of images in\",data_set,\"dataset: \",counter1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls $LOCAL_DATA_DIR/split/test/good_weld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good idea to first look at some examplary images to get a feeling for the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image \n",
    "import os\n",
    "DATA_DIR=os.environ.get('LOCAL_DATA_DIR')\n",
    "\n",
    "w,h = 200,200\n",
    "fig = plt.figure(figsize=(200,200))\n",
    "\n",
    "rows = 1\n",
    "cols = 6 \n",
    "\n",
    "image = ['/split/test/burn_through/0161214-151210-run7image-0967.png',\n",
    "         '/split/test/contamination/0160705-121434-50mmLens added slugimage-0782.png',\n",
    "         '/split/test/good_weld/0160708-115129-50mmLens 200A w.s.Lev12 try joining 5mm Plateimage-0592.png',\n",
    "        '/split/test/high_travel_speed/0160705-113121-50mmLens w.s.154cm.mimage-0145.png',\n",
    "        '/split/test/lack_of_fusion/0160708-145105-50mmLens 350A w.s.Lev16 g.f.20L.m try joining 10mm Plateimage-0682.png',\n",
    "        '/split/test/lack_of_shielding_gas/0160707-111307-50mmLens 200A w.s.11.5cm.m + no shielding gasimage-0155.png']\n",
    "labels = [\"burn_trough\",\"contamination\",\"good_weld\",\"high_travel_speed\",\"lack_of_fusion\",\"lack_of_shielding\"]\n",
    "\n",
    "for i in range(1, cols*rows + 1):\n",
    "    ax = fig.add_subplot(rows, cols,i)\n",
    "    img = Image.open(str(DATA_DIR)+str(image[i-1]))\n",
    "    img = img.resize((200,200), Image.ANTIALIAS)    \n",
    "    plt.imshow(img, cmap='gray')\n",
    "    ax.set_title(labels[i-1], fontsize=150)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Download pretrained models <a class=\"anchor\" id=\"head-1-2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will use NGC CLI to get the pre-trained models. For more details, go to ngc.nvidia.com and click the SETUP on the navigation bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing NGC CLI on the local machine.\n",
    "## Download and install\n",
    "%env CLI=ngccli_reg_linux.zip\n",
    "!mkdir -p $LOCAL_PROJECT_DIR/ngccli\n",
    "\n",
    "# Remove any previously existing CLI installations\n",
    "!rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n",
    "!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $LOCAL_PROJECT_DIR/ngccli\n",
    "!unzip -u \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n",
    "!rm $LOCAL_PROJECT_DIR/ngccli/*.zip \n",
    "os.environ[\"PATH\"]=\"{}/ngccli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ngc registry model list nvidia/tlt_pretrained_classification:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/pretrained_resnet18/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $LOCAL_EXPERIMENT_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $LOCAL_EXPERIMENT_DIR/pretrained_resnet18/tlt_pretrained_classification_vresnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull pretrained model from NGC\n",
    "!ngc registry model download-version nvidia/tlt_pretrained_classification:resnet18 --dest $LOCAL_EXPERIMENT_DIR/pretrained_resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Check that model is downloaded into dir.\")\n",
    "!ls -l $LOCAL_EXPERIMENT_DIR/pretrained_resnet18/tlt_pretrained_classification_vresnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Provide training specfication <a class=\"anchor\" id=\"head-3\"></a>\n",
    "* Training dataset\n",
    "* Validation dataset\n",
    "* Pre-trained models\n",
    "* Other training (hyper-)parameters such as batch size, number of epochs, learning rate etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat $LOCAL_SPECS_DIR/classification_spec.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a comparisson we also want to get Resnet with random weight initialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo pip3 install tensorflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run TLT training <a class=\"anchor\" id=\"head-4\"></a>\n",
    "* Provide the sample spec file and the output directory location for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $SPECS_DIR\n",
    "!echo $USER_EXPERIMENT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "!tlt classification train -e $SPECS_DIR/classification_spec.cfg -r $USER_EXPERIMENT_DIR/output -k $KEY --gpus 2 | tee train_RES18PRE_50EPOC_0_006Learn.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker pull nvcr.io/nvidia/tlt-streamanalytics:v3.0-dp-py3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate trained models <a class=\"anchor\" id=\"head-5\"></a>\n",
    "\n",
    "In this step, we assume that the training is complete and the model from the final epoch (`resnet_0XX.tlt`) is available. If you would like to run evaluation on an earlier model, please edit the spec file at `$SPECS_DIR/classification_spec.cfg` to point to the intended model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time    \n",
    "!tlt classification evaluate -e $SPECS_DIR/classification_spec.cfg -k $KEY  | tee eval_RES50PRE_20EPOC_0_006Learn.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Inferences <a class=\"anchor\" id=\"head-9\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the output results of our model on test images, we can use the `tlt-infer` tool. Note that using models trained for higher epochs will usually result in better results. We'll run inference with the directory mode. You can also use the single image mode.\n",
    "We will run the inference on the contamination direcory. If you want you can interchange that with another class. We will then obtain a csv with the results. In this this csv every row has the name of the image, the forecasted label and the classification score for the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the checkpoint epoch number to use for the subsequent steps.\n",
    "# This should be lesser than the number of epochs training has been run for, incase training was interrupted earlier.\n",
    "%env EPOCH=020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt classification inference -e $SPECS_DIR/classification_spec.cfg \\\n",
    "                          -m $USER_EXPERIMENT_DIR/output/weights/resnet_$EPOCH.tlt \\\n",
    "                          -k $KEY -b 32 -d $DATA_DOWNLOAD_DIR/split/test/contamination \\\n",
    "                          -cm $USER_EXPERIMENT_DIR/output_retrain/classmap.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in Getting Started Guide, this outputs a results.csv file in the same directory. Each row represents an image, the predicted class and the classification score. We can use a simple python program to see the visualize the output of csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image \n",
    "import os\n",
    "import csv\n",
    "import random as rd\n",
    "\n",
    "DATA_DIR = os.environ.get('LOCAL_DATA_DIR')\n",
    "DATA_DOWNLOAD_DIR = os.environ.get('DATA_DOWNLOAD_DIR')\n",
    "csv_path = os.path.join(DATA_DIR, 'split', 'test', 'contamination', 'result.csv')\n",
    "\n",
    "with open(csv_path,newline='\\n') as csvfile:\n",
    "    results = list(csv.reader(csvfile))\n",
    "\n",
    "w,h = 200,200\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "columns = 5\n",
    "rows = 1\n",
    "\n",
    "#print(results)\n",
    "for i, index in enumerate(rd.sample(range(1, len(results)), 5)):  \n",
    "    ax = fig.add_subplot(rows, columns,i+1)\n",
    "    img = Image.open(results[index][0].replace(DATA_DOWNLOAD_DIR, DATA_DIR))\n",
    "    img = img.resize((w,h), Image.ANTIALIAS)\n",
    "    plt.imshow(img,cmap='gray')\n",
    "    ax.set_title(results[index][1] + '\\n' + 'Image: ' + str(index) + ' ' +str(round(float(results[index][2]),3)), fontsize=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
